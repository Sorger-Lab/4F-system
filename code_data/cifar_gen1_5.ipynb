{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3563,
     "status": "ok",
     "timestamp": 1623991025067,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "mdg3KtKw7sDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1623991025068,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "QDljiKBQDIxq",
    "outputId": "0e59fbef-372b-4ea4-c3b3-9cd985209667"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1623991025069,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "KJghSdKaFLoh",
    "outputId": "366e5a12-feda-4a73-c216-eb5085553935"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"torch.cuda.set_device(0)\\ntorch.set_default_tensor_type('torch.cuda.FloatTensor')\""
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''torch.cuda.set_device(0)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y0OKixS72DZ"
   },
   "source": [
    "Import dataset (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "b240074c3cbc42d184195f62013b1824",
      "d0ec09a6a4024b60af07dc8a4f2ccb26",
      "e9309b96936044c39741a93bddd5f826",
      "f4bf101087914dcc9caf9419ad6e3b68",
      "59827769f89f425180889a55e66c99aa",
      "5bab50528faf49e8ab5067564e105651",
      "c4bc71b0ef044476b63f5a7e134f10c5",
      "f36cacf7d90648a5809412342a3be483"
     ]
    },
    "executionInfo": {
     "elapsed": 11247,
     "status": "ok",
     "timestamp": 1623991036306,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "XTCKeZr-7yDi",
    "outputId": "7e07e4c3-faba-475e-dff6-65212c8a9eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b240074c3cbc42d184195f62013b1824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "train_transform = Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "'''\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "'''\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, target_transform=None, download=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                          shuffle=True, pin_memory=False, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=test_transform, target_transform=None, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                         shuffle=False, pin_memory=False, num_workers=8)\n",
    "                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcvDoyHi8E5d"
   },
   "source": [
    "Define the network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1623991036307,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "R3lg2K4Q95qV",
    "outputId": "dcc99640-b2ac-42f2-add8-b5a8d36ead72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 18 04:37:15 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1623991036307,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "hvD1VXi8I-D3"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          outputs = net(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "      100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2341,
     "status": "ok",
     "timestamp": 1623995295526,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "iOebmqXb79md",
    "outputId": "108223ad-5334-464d-baf1-2a7fb6a7baf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put net onto GPU\n",
      "FFTconv(\n",
      "  (conv1): FTconvlayer(3, 16, kernel_size=(32, 32), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc01): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (fc02): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (drop_layer): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "class _ConvNd(Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups', 'bias',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']   \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size, stride,\n",
    "                 padding, dilation, transposed, output_padding,\n",
    "                 groups, bias, padding_mode):\n",
    "        super(_ConvNd, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                in_channels, out_channels // groups, *kernel_size))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        init.kaiming_normal_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(_ConvNd, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "class FTconvlayer(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size = 16, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        batch_size = BATCH_SIZE\n",
    "        kernel_size = _pair(kernel_size) \n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(FTconvlayer, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, batch_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "    def quantization_n(self, input, n = 1, max = 1):\n",
    "      intv = max/(2**n-1)\n",
    "      qunt = torch.ceil(torch.mul(input,(1/intv)))  #***use ceil instead of floor for small value of n to make sure that not all weights are modified to zero in the beginning, achieves good accuracy for small n\n",
    "      #the above line divide the whole tensor by the smallest interval (1/2**n-1), which is same as multiply with 2**n-1, then take the floor and finally multiply the whole tensor with the smallest interval\n",
    "      out = torch.mul(qunt,intv)\n",
    "      out = torch.clamp(out, min=0, max=max) #make sure the quantized version lies in the interval 0-1, if it's bigger than one just clamp it at one\n",
    "      return(out)  \n",
    "\n",
    "    def input_quant(self, input, level = 5):\n",
    "        max = torch.max(input)\n",
    "        intv = max/level\n",
    "        qunt = torch.floor(torch.mul(input,(1/intv)))\n",
    "        out = torch.mul(qunt, intv)\n",
    "        out = torch.clamp(out, min=0, max=max)\n",
    "        return(out)\n",
    "    \n",
    "    def weightclamp(self, input):\n",
    "      return input.clamp_(0)\n",
    "\n",
    "    def make_complex(self, x):  #converts a real tensor into complex form by adding one extra dimension to it\n",
    "        #x_i = torch.zeros(x.shape)\n",
    "        x_i = torch.cuda.FloatTensor(x.shape).fill_(0)\n",
    "        y = torch.stack((x,x_i),-1)\n",
    "        return torch.view_as_complex(y)\n",
    "\n",
    "    def neg_complex_exp(self, x):    #since pytorch does not support complex exponential, implemented using euler formula exp(-jx)=cos(-x)+jsin(-x)\n",
    "        x_cos = torch.cos(-x)\n",
    "        x_sin = torch.sin(-x)\n",
    "        x_euler = torch.stack((x_cos, x_sin), -1)\n",
    "        return torch.view_as_complex(x_euler)\n",
    "\n",
    "    def complex_mul(self, x,y):  #this implementation should support broadcasting \n",
    "        #result_r = x[...,0] * y[...,0]-x[...,1] * y[...,1]#real\n",
    "        #result_i = x[...,0] * y[...,1]+x[...,1] * y[...,0]#complex\n",
    "        #result = torch.stack((result_r,result_i),-1)#stack them together to get the result\n",
    "        result = x*y\n",
    "        return result\n",
    "\n",
    "    def conj_transpose(self, x):  #should support broadcasting\n",
    "        x = torch.view_as_real(x)\n",
    "        size = len(x.size())\n",
    "        x_r = x[...,0]\n",
    "        x_i = x[...,1]\n",
    "        x_i_c =-x_i\n",
    "        x_conj = torch.stack((x_r,x_i_c),-1)\n",
    "        x_conj_t = torch.transpose(x_conj, size-3, size-2)#size-1 is the dimension for complex representation\n",
    "        return torch.view_as_complex(x_conj_t)\n",
    "\n",
    "    def roll_n(self, X, axis, n):\n",
    "        f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))\n",
    "        b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))\n",
    "        front = X[f_idx]\n",
    "        back = X[b_idx]\n",
    "        return torch.cat([back, front], axis)   \n",
    "\n",
    "    def batch_fftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size())-2, len(real.size())):\n",
    "            n_shift = real.size(dim)//2\n",
    "            if real.size(dim) % 2 != 0:\n",
    "                n_shift += 1  # for odd-sized images\n",
    "            real = self.roll_n(real, axis=dim, n=n_shift)\n",
    "            imag = self.roll_n(imag, axis=dim, n=n_shift)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "\n",
    "    def batch_ifftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size()) - 1, len(real.size())-3, -1):\n",
    "            real = self.roll_n(real, axis=dim, n=real.size(dim)//2)\n",
    "            imag = self.roll_n(imag, axis=dim, n=imag.size(dim)//2)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "    def propTF(self, u1,L,lambdaa,z):\n",
    "        batch,M,N = u1.shape\n",
    "        dx = L/M\n",
    "        fx = torch.arange(-1/(2*dx),1/(2*dx), 1/L).cuda()\n",
    "        FX,FY = torch.meshgrid(fx,fx)\n",
    "        #H = torch.exp(-1j*math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        H = self.neg_complex_exp(math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        #H = self.batch_fftshift2d(H)\n",
    "        H = torch.fft.fftshift(H)\n",
    "        U1 = torch.fft.fft2(torch.fft.fftshift(u1)) #so the U1's dimension is 4, H is 3 so complex_mul needs to support broadcasting\n",
    "        U2 = self.complex_mul(H,U1)\n",
    "        u2 = torch.fft.ifftshift(torch.fft.ifft2(U2)) #this step is redundent, can be removed in actual implementation\n",
    "        return u2\n",
    "\n",
    "\n",
    "    def seidel_5(self, u0, v0, X, Y, wd, w040, w131, w222, w220, w311):\n",
    "        beta = math.atan2(u0,v0)\n",
    "        u0r=math.sqrt(u0**2+v0**2)\n",
    "        Xr=X*math.cos(beta)+Y*math.sin(beta)\n",
    "        Yr=-X*math.sin(beta)+Y*math.cos(beta)\n",
    "        rho2=Xr**2+Yr**2\n",
    "        w=wd*rho2+w040*rho2**2+w131*u0r*rho2*Xr+ w222*u0r**2*Xr**2+w220*u0r**2*rho2+w311*math.pow(u0r,3)*Xr\n",
    "        return w\n",
    "\n",
    "    def circ(self, r):\n",
    "        out = torch.abs(r)<=1\n",
    "        return out\n",
    "\n",
    "    ''' for block mean pytorch does not support reshape using 'F' ordering, so use normal reshape and then permute'''\n",
    "    def blockmean_batch(self, X, V, W):\n",
    "        S=X.shape\n",
    "        B1 = S[0]\n",
    "        B2 = S[1]\n",
    "        M = int(S[2] - S[2]%V)\n",
    "        N = int(S[3] - S[3]%W)\n",
    "        if(M*N == 0):\n",
    "            Y = X\n",
    "            return Y\n",
    "        MV = int(M/V)  \n",
    "        NW = int(N/W)\n",
    "        #XM = np.reshape(X[0:M, 0:N, :],(V, MV, W, NW, -1))\n",
    "        #XM =  X[0:M, 0:N].reshape(V, MV, W, NW, order=\"F\")\n",
    "        XM = X[:,:,0:M, 0:N].permute(0,1,3,2).reshape([B1,B2,NW, W, MV, V]).permute(0,1,5,4,3,2)\n",
    "        #three version of Y in matlab function depends on differen type of inputs, here stick to the double case\n",
    "        Y = torch.sum(torch.sum(XM,2),3) * (1/(V*W))\n",
    "        return Y\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "        size = input.shape[-1]\n",
    "        start = int((size-4*img_size)/2)\n",
    "        end = start + 4*img_size\n",
    "        output = input[:,:,start:end,start:end]\n",
    "        return output\n",
    "    \n",
    "    def input_pad(self,input,padsize):\n",
    "      input_size = input.shape[2]\n",
    "      pad_size_x = int((padsize-input_size)/2)\n",
    "      pad_size_y = int((padsize-input_size)/2)\n",
    "      p2d = (pad_size_x, pad_size_y, pad_size_x, pad_size_y)\n",
    "      input_pad = F.pad(input, p2d, \"constant\", 0)\n",
    "      return input_pad \n",
    "\n",
    "    def input_adjust(self, input):\n",
    "      input = torch.mul(input,5)\n",
    "      input = torch.floor(input)\n",
    "      output = torch.clamp(input,min=0,max=1)\n",
    "      return(output)\n",
    "\n",
    "    def evenkernel(self, input):\n",
    "      uptri = torch.triu(input,diagonal = 1)\n",
    "      downtri = torch.flip(torch.triu(input,diagonal = 1),[1,2])\n",
    "      result = uptri+downtri\n",
    "      return result\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "      size = input.shape[-1]\n",
    "      start = int((size-img_size)/2)\n",
    "      end = start + img_size\n",
    "      output = input[:,:,start:end,start:end]\n",
    "      return output\n",
    "    \n",
    "    def norm(self,input):\n",
    "      size = input.shape\n",
    "      output = torch.cuda.FloatTensor(size).fill_(0)\n",
    "      for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "          orig = input[i,j,:,:]\n",
    "          maxi = torch.max(orig)\n",
    "          mini = torch.min(orig)\n",
    "          output[i,j,:,:] = (orig-mini)/(maxi-mini)\n",
    "      return output\n",
    "\n",
    "    def kernel_even(self,input):\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = input + input_transpose\n",
    "      return input\n",
    "\n",
    "    def kernel_hpf_even(self, input, amount):\n",
    "      # make the center part of the weight to be zero and make the filter symmetrical\n",
    "      # This function should be placed before quantization\n",
    "      # amount is the size of the area that are set to 0\n",
    "      mid = int(input.shape[2]/2)\n",
    "      input[:,mid-amount:mid+amount,mid-amount:mid+amount] = 0 #set the center part to be zero\n",
    "\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = (input + input_transpose)/2\n",
    "      #even = input\n",
    "      return input\n",
    "\n",
    "    def accurate_model_forward(self, input, weight):\n",
    "        err = 1e-8 #define a very small error term to aviod nan loss due to abs\n",
    "        #print(input[0, ...])\n",
    "        #code to quantize the input to certain intervals to simulate dmd\n",
    "        #with torch.no_grad():\n",
    "          #input = self.input_quant(input)\n",
    "        res = input.shape[2]\n",
    "        xx = res\n",
    "        yy = xx \n",
    "        w = 32    \n",
    "        #output_arr = torch.empty(32,16,w,w) #output dimension of the ftconv layer, hard-coded for easier implementation, 28 for MNIST and 32 for CIFAR\n",
    "        n_filter_actual = int(self.out_channels/2)\n",
    "        output_full = torch.cuda.FloatTensor(BATCH_SIZE,self.out_channels,w,w).fill_(0)\n",
    "        output_sub = torch.cuda.FloatTensor(BATCH_SIZE, int(self.out_channels/2), w, w).fill_(0)\n",
    "        # Define unit matrix DMD \n",
    "        '''unitmatrix = torch.cuda.FloatTensor(17, 17).fill_(0)\n",
    "        unitmatrix[1:16,1:16]=1\n",
    "        unitmatrix[7:10,7:10]=0\n",
    "        idledmd = unitmatrix.repeat(xx,yy)\n",
    "        '''\n",
    "        idledmd = torch.cuda.FloatTensor(res, res).fill_(1)\n",
    "        M,N = idledmd.shape\n",
    "        L1=1.90e-2*xx/res\n",
    "        L2=1.09e-2*yy/res\n",
    "        du=L1/M\n",
    "        dv=L2/N\n",
    "        lambdaa = 0.633e-6\n",
    "        k=2*math.pi/lambdaa\n",
    "        \n",
    "        '''Lens Diffraction (Aperture) and Aberration'''\n",
    "        fu = torch.arange(-1/(2*du),1/(2*du),1/L1)\n",
    "        #fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        Dxp = 5e-2\n",
    "        wxp = Dxp/2\n",
    "        zxp = 200e-3\n",
    "        lz = lambdaa*zxp\n",
    "        u0 = 0\n",
    "        v0 = 0\n",
    "        f0 = wxp/(lambdaa*zxp)\n",
    "        '''Lens parameter for aberration (Seidel coefficients), wavefront alteration from spherical waves'''\n",
    "        wd=0*lambdaa\n",
    "        w040=4.963*lambdaa\n",
    "        w131=2.637*lambdaa\n",
    "        w222=9.025*lambdaa\n",
    "        w220=7.536*2*lambdaa\n",
    "        w311=0.157*12*lambdaa\n",
    "        \n",
    "        Fu,Fv = torch.meshgrid(fu,fv)\n",
    "        Fu = torch.transpose(Fu,0,1)\n",
    "        Fv = torch.transpose(Fv,0,1)\n",
    "        W = self.seidel_5(u0,v0,-lz*Fu/wxp,-lz*Fv/wxp,wd,w040,w131,w222,w220,w311).cuda() #same as the matlab calculation\n",
    "        #H = circ(torch.sqrt(Fu**2 + Fv**2)/f0)*torch.exp(-1j*k*W)#same as matlab calculation\n",
    "        H = self.complex_mul(self.make_complex(self.circ(torch.sqrt(Fu**2 + Fv**2)/f0).float().cuda()),self.neg_complex_exp(k*W))\n",
    "#-----------------------from here is the actual training, before the loop is basically constants/parameters genreation, which does not needs to be backproped     \n",
    "        for c_in in range(input.shape[1]): # iters for number of input channels\n",
    "            signal = input[:,c_in,:,:] #the dimension of signal is 3, with one batch dimension\n",
    "            weight_raw = weight[:,c_in,:,:] #the dimension of weights are now 3\n",
    "            #apply high pass filter and make kernel even\n",
    "            #weight_raw.data = self.kernel_even(weight_raw.data)\n",
    "            weight_raw.data = self.kernel_hpf_even(weight_raw.data,3)\n",
    "            weight_raw.data = self.quantization_n(weight_raw.data, 1, 1)\n",
    "            #print(weight_raw[0,...])\n",
    "            #weight_raw.data = torch.clamp(weight_raw.data, min=0, max=1)\n",
    "            '''interleave dimension needs to be changed since now the first dimension is batch dimension'''\n",
    "            #sw = torch.repeat_interleave(signal,4, dim=1)\n",
    "            #sw = torch.repeat_interleave(sw,4, dim=2)\n",
    "            dmd_1 = self.make_complex(self.input_pad(signal,res)) #dimension of dmd1 is now 4, first dimension is now batch dimension, so propTF needs to be changed accordingly\n",
    "            '''interleave dimension also needs to be changed here'''\n",
    "            #kk = torch.repeat_interleave(weight_raw,20, dim=1)#due to alignment reason, adjust the minimum unit of kernels to 50*50 pixles, so effective kernek size is 84*84\n",
    "            #kk = torch.repeat_interleave(kk,20, dim=2)\n",
    "            #dmd_kk = self.input_pad(kk,res)\n",
    "            #dmd_kk = self.evenkernel(weight_raw)\n",
    "            #dmd_kk = weight_raw\n",
    "            \n",
    "            #now need to implement propTF function\n",
    "            u2 = self.propTF(dmd_1,1.9e-2, lambdaa, 1.9e-2)\n",
    "            #u2 = dmd_1\n",
    "            \n",
    "            '''Fourier Transform after first lens'''\n",
    "            Gg = torch.fft.fftshift(torch.fft.fft2(u2))\n",
    "            Gi = self.complex_mul(Gg,self.conj_transpose(H))\n",
    "            Gi = Gi.unsqueeze(1)\n",
    "            \n",
    "            '''dot product in the fourier plane'''\n",
    "            Gii = self.complex_mul(Gi,self.make_complex(weight_raw))\n",
    "            '''Then get the result in real space'''\n",
    "            Grs = torch.fft.ifft2(torch.fft.ifftshift(Gii))\n",
    "            Grs = torch.view_as_real(Grs)\n",
    "            #Ii = torch.abs(Grs)**2\n",
    "            Ii = torch.sqrt(Grs[...,0]**2+Grs[...,1]**2+err)\n",
    "            #Ii = Grs[...,0]**2+Grs[...,1]**2 #take the squre to represent light intensity\n",
    "            op_abs = Ii#op_abs = self.extract_result(Ii,32)\n",
    "            output_full += op_abs\n",
    "        #output_sub = output_full[:, 0:n_filter_actual, :, :] - output_full[:, n_filter_actual:, :, :]\n",
    "        return output_full\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.accurate_model_forward(input, self.weight)\n",
    "        \n",
    "        #shape of input is [32,1,28,28], shape of kernel(weight) is [16,1,5,5]\n",
    "\n",
    "\n",
    "class FFTconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTconv, self).__init__()\n",
    "        self.conv1 = FTconvlayer(3, 16, 32) #outdimension should be [32,16,28,28] (1/3,:,5/28/32) depends on whether training in fourier domain or no\n",
    "        #self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.fc01 = nn.Linear(16 * 16 * 16, 256)\n",
    "        self.fc02 = nn.Linear(256, 10)\n",
    "        self.drop_layer = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.bn1(self.pool1(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 16 * 16)\n",
    "        #x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc01(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        x = self.fc02(x)\n",
    "        return x\n",
    "\n",
    "net = FFTconv()    \n",
    "\n",
    "\n",
    "if device:\n",
    "    net.to(device)\n",
    "    #torch.backends.cudnn.benchmark = True\n",
    "    print(\"put net onto GPU\")\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1623995296525,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "c0LeM9VV9sJZ"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #default learning rate for adam is 0.001\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, init_lr, freq):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every n epochs\"\"\"\n",
    "    lr = init_lr * (0.3 ** (epoch // freq))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413837,
     "status": "ok",
     "timestamp": 1623995711881,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "RamA6QAh9sot",
    "outputId": "6f63bcd6-e450-450b-ef62-1c07ece71629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.091\n",
      "[1,   400] loss: 1.719\n",
      "[1,   600] loss: 1.585\n",
      "Accuracy of the network on the 10000 test images: 40 %\n",
      "Training epoch ...\n",
      "[2,   200] loss: 1.479\n",
      "[2,   400] loss: 1.431\n",
      "[2,   600] loss: 1.402\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Training epoch ...\n",
      "[3,   200] loss: 1.371\n",
      "[3,   400] loss: 1.349\n",
      "[3,   600] loss: 1.345\n",
      "Accuracy of the network on the 10000 test images: 46 %\n",
      "Training epoch ...\n",
      "[4,   200] loss: 1.298\n",
      "[4,   400] loss: 1.314\n",
      "[4,   600] loss: 1.314\n",
      "Accuracy of the network on the 10000 test images: 47 %\n",
      "Training epoch ...\n",
      "[5,   200] loss: 1.281\n",
      "[5,   400] loss: 1.270\n",
      "[5,   600] loss: 1.279\n",
      "Accuracy of the network on the 10000 test images: 47 %\n",
      "Training epoch ...\n",
      "[6,   200] loss: 1.257\n",
      "[6,   400] loss: 1.237\n",
      "[6,   600] loss: 1.244\n",
      "Accuracy of the network on the 10000 test images: 49 %\n",
      "Training epoch ...\n",
      "[7,   200] loss: 1.236\n",
      "[7,   400] loss: 1.239\n",
      "[7,   600] loss: 1.226\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Training epoch ...\n",
      "[8,   200] loss: 1.208\n",
      "[8,   400] loss: 1.201\n",
      "[8,   600] loss: 1.204\n",
      "Accuracy of the network on the 10000 test images: 51 %\n",
      "Training epoch ...\n",
      "[9,   200] loss: 1.201\n",
      "[9,   400] loss: 1.199\n",
      "[9,   600] loss: 1.177\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Training epoch ...\n",
      "[10,   200] loss: 1.181\n",
      "[10,   400] loss: 1.173\n",
      "[10,   600] loss: 1.180\n",
      "Accuracy of the network on the 10000 test images: 52 %\n",
      "Training epoch ...\n",
      "[11,   200] loss: 1.122\n",
      "[11,   400] loss: 1.113\n",
      "[11,   600] loss: 1.092\n",
      "Accuracy of the network on the 10000 test images: 52 %\n",
      "Training epoch ...\n",
      "[12,   200] loss: 1.094\n",
      "[12,   400] loss: 1.089\n",
      "[12,   600] loss: 1.078\n",
      "Accuracy of the network on the 10000 test images: 52 %\n",
      "Training epoch ...\n",
      "[13,   200] loss: 1.094\n",
      "[13,   400] loss: 1.080\n",
      "[13,   600] loss: 1.091\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[14,   200] loss: 1.064\n",
      "[14,   400] loss: 1.067\n",
      "[14,   600] loss: 1.094\n",
      "Accuracy of the network on the 10000 test images: 51 %\n",
      "Training epoch ...\n",
      "[15,   200] loss: 1.082\n",
      "[15,   400] loss: 1.053\n",
      "[15,   600] loss: 1.079\n",
      "Accuracy of the network on the 10000 test images: 52 %\n",
      "Training epoch ...\n",
      "[16,   200] loss: 1.058\n",
      "[16,   400] loss: 1.079\n",
      "[16,   600] loss: 1.079\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[17,   200] loss: 1.049\n",
      "[17,   400] loss: 1.086\n",
      "[17,   600] loss: 1.060\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[18,   200] loss: 1.058\n",
      "[18,   400] loss: 1.051\n",
      "[18,   600] loss: 1.066\n",
      "Accuracy of the network on the 10000 test images: 52 %\n",
      "Training epoch ...\n",
      "[19,   200] loss: 1.066\n",
      "[19,   400] loss: 1.044\n",
      "[19,   600] loss: 1.055\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[20,   200] loss: 1.054\n",
      "[20,   400] loss: 1.039\n",
      "[20,   600] loss: 1.055\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[21,   200] loss: 1.023\n",
      "[21,   400] loss: 1.012\n",
      "[21,   600] loss: 1.024\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[22,   200] loss: 1.019\n",
      "[22,   400] loss: 1.024\n",
      "[22,   600] loss: 1.010\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[23,   200] loss: 1.004\n",
      "[23,   400] loss: 1.017\n",
      "[23,   600] loss: 1.010\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[24,   200] loss: 1.002\n",
      "[24,   400] loss: 1.018\n",
      "[24,   600] loss: 1.018\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[25,   200] loss: 1.017\n",
      "[25,   400] loss: 1.002\n",
      "[25,   600] loss: 0.998\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[26,   200] loss: 1.006\n",
      "[26,   400] loss: 1.010\n",
      "[26,   600] loss: 1.006\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[27,   200] loss: 1.001\n",
      "[27,   400] loss: 1.013\n",
      "[27,   600] loss: 1.006\n",
      "Accuracy of the network on the 10000 test images: 53 %\n",
      "Training epoch ...\n",
      "[28,   200] loss: 1.003\n",
      "[28,   400] loss: 1.007\n",
      "[28,   600] loss: 0.997\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[29,   200] loss: 1.007\n",
      "[29,   400] loss: 0.992\n",
      "[29,   600] loss: 1.000\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training epoch ...\n",
      "[30,   200] loss: 1.021\n",
      "[30,   400] loss: 0.993\n",
      "[30,   600] loss: 1.003\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    print('Training epoch ...')\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    adjust_learning_rate(optimizer, epoch, 0.001, 10)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    test()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1623801477450,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "a1BDdPoiBz61",
    "outputId": "470f2878-88f9-44b2-c6db-8529066e0f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[9, 0, 1, 7, 4, 3],\n",
      "         [4, 9, 5, 0, 7, 3],\n",
      "         [9, 2, 0, 0, 4, 3],\n",
      "         [7, 3, 0, 0, 6, 7],\n",
      "         [2, 0, 9, 6, 3, 8],\n",
      "         [2, 6, 7, 9, 8, 1]],\n",
      "\n",
      "        [[5, 3, 9, 3, 1, 5],\n",
      "         [3, 8, 2, 2, 6, 1],\n",
      "         [3, 1, 0, 0, 7, 7],\n",
      "         [4, 6, 0, 0, 5, 5],\n",
      "         [4, 9, 5, 0, 2, 2],\n",
      "         [0, 1, 1, 5, 8, 3]],\n",
      "\n",
      "        [[7, 4, 2, 6, 7, 4],\n",
      "         [4, 3, 8, 0, 7, 0],\n",
      "         [0, 0, 0, 0, 8, 4],\n",
      "         [8, 2, 0, 0, 6, 3],\n",
      "         [0, 6, 3, 5, 4, 5],\n",
      "         [3, 0, 9, 2, 3, 4]]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.randint(10,(3,5,5))\n",
    "test_tp = torch.transpose(test,1,2)\n",
    "output = test+test_tp\n",
    "#print(output)\n",
    "test_2 = torch.randint(10,(3,6,6))\n",
    "test_2[:,2:4,2:4] = 0\n",
    "print(test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1623993517926,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "cs4SMkFFb59B",
    "outputId": "9c9c02e8-922b-404e-bd58-888c53f9c493"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 54 %\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr7BIzPL-BkM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623995711881,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "Y6mdLKLZ-QG-"
   },
   "outputs": [],
   "source": [
    "#functions required to generate final weights\n",
    "def quantization_n(input, n = 1, max = 1):\n",
    "  intv = max/(2**n-1)\n",
    "  qunt = torch.ceil(torch.mul(input,(1/intv)))  #***use ceil instead of floor for small value of n to make sure that not all weights are modified to zero in the beginning, achieves good accuracy for small n\n",
    "  #the above line divide the whole tensor by the smallest interval (1/2**n-1), which is same as multiply with 2**n-1, then take the floor and finally multiply the whole tensor with the smallest interval\n",
    "  out = torch.mul(qunt,intv)\n",
    "  out = torch.clamp(out, min=0, max=max) #make sure the quantized version lies in the interval 0-1, if it's bigger than one just clamp it at one\n",
    "  return(out) \n",
    "\n",
    "def kernel_even(input):\n",
    "  dim = len(input.shape)\n",
    "  print(dim)\n",
    "  input_transpose = torch.transpose(input,dim-2,dim-1)\n",
    "  input = (input + input_transpose)/2\n",
    "  return input\n",
    "\n",
    "def kernel_hpf_even(input, amount):\n",
    "  # make the center part of the weight to be zero and make the filter symmetrical\n",
    "  # This function should be placed before quantization\n",
    "  # amount is the size of the area that are set to 0\n",
    "  mid = int(input.shape[2]/2)\n",
    "  dim = len(input.shape)\n",
    "  input[...,mid-amount:mid+amount,mid-amount:mid+amount] = 0 #set the center part to be zero\n",
    "  print(dim)\n",
    "  input_transpose = torch.transpose(input, dim-2,dim-1)\n",
    "  input = (input + input_transpose)/2\n",
    "  #even = input\n",
    "  return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1623995711882,
     "user": {
      "displayName": "SHURUI LI",
      "photoUrl": "",
      "userId": "04216126027368487065"
     },
     "user_tz": 420
    },
    "id": "Nj9tT_Vlq-Iy",
    "outputId": "6b43fd88-b986-4a6d-b6d1-5aa6ac3049a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "weights = torch.clone(net.state_dict()['conv1.weight'])\n",
    "weights = kernel_hpf_even(weights,3)\n",
    "#weights = kernel_even(weights)\n",
    "weights = quantization_n(weights,1,1)\n",
    "weights_np = weights.cpu().numpy()\n",
    "np.save('cifar_even_hpf6',weights_np)\n",
    "print(weights[7,0,12:20,12:20])\n",
    "#print(net.state_dict()['conv1.weight'][10,...])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cifar_gen1_5.ipynb",
   "provenance": [
    {
     "file_id": "1wZENm6khC3gxza9mNfWExb5EvQkl1MT-",
     "timestamp": 1623792097895
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "59827769f89f425180889a55e66c99aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5bab50528faf49e8ab5067564e105651": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b240074c3cbc42d184195f62013b1824": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e9309b96936044c39741a93bddd5f826",
       "IPY_MODEL_f4bf101087914dcc9caf9419ad6e3b68"
      ],
      "layout": "IPY_MODEL_d0ec09a6a4024b60af07dc8a4f2ccb26"
     }
    },
    "c4bc71b0ef044476b63f5a7e134f10c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0ec09a6a4024b60af07dc8a4f2ccb26": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9309b96936044c39741a93bddd5f826": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bab50528faf49e8ab5067564e105651",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59827769f89f425180889a55e66c99aa",
      "value": 170498071
     }
    },
    "f36cacf7d90648a5809412342a3be483": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4bf101087914dcc9caf9419ad6e3b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f36cacf7d90648a5809412342a3be483",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c4bc71b0ef044476b63f5a7e134f10c5",
      "value": " 170499072/? [00:20&lt;00:00, 8230848.28it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
